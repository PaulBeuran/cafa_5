{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from cafa_5.dataset import CAFA5Dataset, collate_tok_mult_out_batch\n",
    "from cafa_5.transform import CharTokenizer, MultiOutputBinarizer\n",
    "from cafa_5.model import CAFA5Transformer, StepLRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and fit transforms\n",
    "cafa_5_train_data = CAFA5Dataset(\n",
    "    \"../kaggle/input/cafa-5-protein-function-prediction/Train/train_sequences.fasta\",\n",
    "    \"../kaggle/input/cafa-5-protein-function-prediction/Train/train_terms.tsv\",\n",
    "    \"../kaggle/input/cafa-5-protein-function-prediction/IA.txt\",\n",
    "    CharTokenizer(max_size=1024, padding=True),\n",
    "    MultiOutputBinarizer()\n",
    ")\n",
    "cafa_5_train_data.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'P20536'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cafa_5_train_data.prots_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNSVTVSHAPYTITYHDDWEPVMSQLVEFYNEVASWLLRDETSPIPDKFFIQLKQPLRNKRVCVCGIDPYPKDGTGVPFESPNFTKKSIKEIASSISRLTGVIDYKGYNLNIIDGVIPWNYYLSCKLGETKSHAIYWDKISKLLLQHITKHVSVLYCLGKTDFSNIRAKLESPVTTIVGYHPAARDRQFEKDRSFEIINVLLELDNKVPINWAQGFIY\n"
     ]
    }
   ],
   "source": [
    "print(cafa_5_train_data.prots_seqs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CAFA5Transformer(\n",
       "  (embedding): Embedding(28, 512)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (feed_forward): FCNN(\n",
       "    (hidden_activation): ReLU()\n",
       "    (output_activation): Sigmoid()\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=43248, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/445 [00:00<?, ?it/s]]/home/pbeuran/repositories/cafa_5/cafa_5/dataset.py:105: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995026/work/aten/src/ATen/NestedTensorImpl.cpp:177.)\n",
      "  \"input_ids\": torch.nested.to_padded_tensor(torch.nested.nested_tensor(sequences_input_ids), 0).squeeze(),\n",
      "- Epoch: 0, Mode: train, Loss: 0.011399 (0.007525),F-score : 0.240419 (0.310132): 100%|██████████| 4001/4001 [05:54<00:00, 11.29it/s]\n",
      "- Epoch: 0, Mode: validation, Loss: 0.009455, F-score : 0.279684: 100%|██████████| 445/445 [06:13<00:00,  1.19it/s]  \n",
      "- Epoch: 1, Mode: train, Loss: 0.009321 (0.007506),F-score : 0.243201 (0.250872): 100%|██████████| 4001/4001 [05:53<00:00, 11.31it/s]\n",
      "- Epoch: 1, Mode: validation, Loss: 0.009394, F-score : 0.244667: 100%|██████████| 445/445 [06:12<00:00,  1.19it/s]  \n",
      "- Epoch: 2, Mode: train, Loss: 0.009255 (0.007489),F-score : 0.245312 (0.267309): 100%|██████████| 4001/4001 [05:53<00:00, 11.33it/s]\n",
      "- Epoch: 2, Mode: validation, Loss: 0.009344, F-score : 0.257933: 100%|██████████| 445/445 [06:11<00:00,  1.20it/s]  \n",
      "- Epoch: 3, Mode: train, Loss: 0.009204 (0.007403),F-score : 0.245954 (0.256502): 100%|██████████| 4001/4001 [05:52<00:00, 11.35it/s]\n",
      "- Epoch: 3, Mode: validation, Loss: 0.0093, F-score : 0.254098: 100%|██████████| 445/445 [06:11<00:00,  1.20it/s]    \n",
      "- Epoch: 4, Mode: train, Loss: 0.009161 (0.007459),F-score : 0.246781 (0.260906): 100%|██████████| 4001/4001 [05:53<00:00, 11.33it/s]\n",
      "- Epoch: 4, Mode: validation, Loss: 0.009264, F-score : 0.242384: 100%|██████████| 445/445 [06:11<00:00,  1.20it/s]  \n",
      "- Epoch: 5, Mode: train, Loss: 0.009122 (0.007321),F-score : 0.247196 (0.262899): 100%|██████████| 4001/4001 [05:53<00:00, 11.33it/s]\n",
      "- Epoch: 5, Mode: validation, Loss: 0.009261, F-score : 0.240188: 100%|██████████| 445/445 [06:11<00:00,  1.20it/s]  \n",
      "- Epoch: 6, Mode: train, Loss: 0.009089 (0.007437),F-score : 0.246955 (0.262808): 100%|██████████| 4001/4001 [05:52<00:00, 11.35it/s]\n",
      "- Epoch: 6, Mode: validation, Loss: 0.009234, F-score : 0.238816: 100%|██████████| 445/445 [06:11<00:00,  1.20it/s]  \n",
      "- Epoch: 7, Mode: train, Loss: 0.009055 (0.007317),F-score : 0.247486 (0.268693): 100%|██████████| 4001/4001 [05:52<00:00, 11.34it/s]\n",
      "- Epoch: 7, Mode: validation, Loss: 0.009216, F-score : 0.234511: 100%|██████████| 445/445 [06:11<00:00,  1.20it/s]  \n"
     ]
    }
   ],
   "source": [
    "cafa_5_transformer = CAFA5Transformer(\n",
    "    amino_acids_vocab = cafa_5_train_data.seq_transform.vocab,\n",
    "    go_codes_vocab = cafa_5_train_data.go_codes_ids,\n",
    "    embedding_kwargs = {\n",
    "        \"embedding_dim\": 512,\n",
    "    },\n",
    "    transformer_kwargs = {\n",
    "        \"num_encoder_layers\": 1\n",
    "    },\n",
    "    feed_forward_kwargs = {\n",
    "        \"num_layers\": 0,\n",
    "        \"hidden_size\": 2048,\n",
    "        \"hidden_activation\": torch.nn.ReLU(),\n",
    "        \"dropout\": 0.1\n",
    "    }\n",
    ")\n",
    "cafa_5_transformer.to(\"cuda\")\n",
    "display(cafa_5_transformer)\n",
    "\n",
    "optimizer_kwargs = {\n",
    "    \"betas\": (0.9, 0.999),\n",
    "    \"eps\": 1e-08,\n",
    "    \"amsgrad\": True,\n",
    "}\n",
    "cafa_5_transformer.fit(\n",
    "    cafa_5_train_data,\n",
    "    epochs=8,\n",
    "    batch_size=32,\n",
    "    collate_fn = collate_tok_mult_out_batch,\n",
    "    loss_fn = ClassWeightedBCELoss(torch.tensor(cafa_5_train_data.go_codes_info_accr_weights, device=\"cuda\")),\n",
    "    optimizer_type = torch.optim.Adam,\n",
    "    optimizer_kwargs = optimizer_kwargs,\n",
    "    lr_scheduler_type = StepLRScheduler,\n",
    "    lr_scheduler_kwargs = {\n",
    "        \"d_model\": 512\n",
    "    },\n",
    "    verbose = True,\n",
    "    validation_size = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CAFA5Transformer(\n",
       "  (embedding): Embedding(28, 512)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (feed_forward): FCNN(\n",
       "    (hidden_activation): ReLU()\n",
       "    (output_activation): Sigmoid()\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=43248, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Epoch: 0, Mode: train, Loss: 0.011542 (0.007283),F-score : 0.240606 (0.237337): 100%|██████████| 4001/4001 [09:23<00:00,  7.10it/s]\n",
      "- Epoch: 0, Mode: validation, Loss: 0.009188, F-score : 0.21535: 100%|██████████| 445/445 [09:43<00:00,  1.31s/it]   \n",
      "- Epoch: 1, Mode: train, Loss: 0.00936 (0.007191),F-score : 0.24058 (0.232324): 100%|██████████| 4001/4001 [09:23<00:00,  7.10it/s]  \n",
      "- Epoch: 1, Mode: validation, Loss: 0.009149, F-score : 0.220481: 100%|██████████| 445/445 [09:42<00:00,  1.31s/it]  \n",
      "- Epoch: 2, Mode: train, Loss: 0.009315 (0.007084),F-score : 0.239609 (0.249423): 100%|██████████| 4001/4001 [09:23<00:00,  7.10it/s]\n",
      "- Epoch: 2, Mode: validation, Loss: 0.009116, F-score : 0.218441: 100%|██████████| 445/445 [09:43<00:00,  1.31s/it]  \n",
      "- Epoch: 3, Mode: train, Loss: 0.009273 (0.007153),F-score : 0.238978 (0.254998): 100%|██████████| 4001/4001 [09:24<00:00,  7.09it/s]\n",
      "- Epoch: 3, Mode: validation, Loss: 0.009072, F-score : 0.230299: 100%|██████████| 445/445 [09:44<00:00,  1.31s/it]  \n",
      "- Epoch: 4, Mode: train, Loss: 0.009251 (0.007038),F-score : 0.238593 (0.312786): 100%|██████████| 4001/4001 [09:24<00:00,  7.09it/s]\n",
      "- Epoch: 4, Mode: validation, Loss: 0.009049, F-score : 0.243544: 100%|██████████| 445/445 [09:43<00:00,  1.31s/it]  \n",
      "- Epoch: 5, Mode: train, Loss: 0.00923 (0.007108),F-score : 0.237272 (0.263868): 100%|██████████| 4001/4001 [09:24<00:00,  7.09it/s] \n",
      "- Epoch: 5, Mode: validation, Loss: 0.009072, F-score : 0.242232: 100%|██████████| 445/445 [09:43<00:00,  1.31s/it]  \n",
      "- Epoch: 6, Mode: train, Loss: 0.009215 (0.007018),F-score : 0.236571 (0.257884): 100%|██████████| 4001/4001 [09:24<00:00,  7.08it/s]\n",
      "- Epoch: 6, Mode: validation, Loss: 0.009066, F-score : 0.237296: 100%|██████████| 445/445 [09:44<00:00,  1.31s/it]  \n",
      "- Epoch: 7, Mode: train, Loss: 0.009197 (0.006999),F-score : 0.236779 (0.250065): 100%|██████████| 4001/4001 [09:24<00:00,  7.08it/s]\n",
      "- Epoch: 7, Mode: validation, Loss: 0.009053, F-score : 0.232774: 100%|██████████| 445/445 [09:44<00:00,  1.31s/it]  \n",
      "- Epoch: 8, Mode: train, Loss: 0.009181 (0.006915),F-score : 0.237143 (0.25408): 100%|██████████| 4001/4001 [09:24<00:00,  7.08it/s] \n",
      "- Epoch: 8, Mode: validation, Loss: 0.009025, F-score : 0.246963: 100%|██████████| 445/445 [09:44<00:00,  1.31s/it]  \n",
      "- Epoch: 9, Mode: train, Loss: 0.009174 (0.006935),F-score : 0.236983 (0.256253): 100%|██████████| 4001/4001 [09:25<00:00,  7.08it/s]\n",
      "- Epoch: 9, Mode: validation, Loss: 0.009089, F-score : 0.241607: 100%|██████████| 445/445 [09:44<00:00,  1.31s/it]  \n",
      "- Epoch: 10, Mode: train, Loss: 0.009166 (0.006933),F-score : 0.237274 (0.249223): 100%|██████████| 4001/4001 [09:24<00:00,  7.09it/s]\n",
      "- Epoch: 10, Mode: validation, Loss: 0.00906, F-score : 0.239619: 100%|██████████| 445/445 [09:43<00:00,  1.31s/it]   \n",
      "- Epoch: 11, Mode: train, Loss: 0.009158 (0.006979),F-score : 0.23724 (0.240078): 100%|██████████| 4001/4001 [09:23<00:00,  7.10it/s] \n",
      "- Epoch: 11, Mode: validation, Loss: 0.009033, F-score : 0.241341: 100%|██████████| 445/445 [09:42<00:00,  1.31s/it]  \n",
      "- Epoch: 12, Mode: train, Loss: 0.009151 (0.006953),F-score : 0.237126 (0.240706): 100%|██████████| 4001/4001 [09:23<00:00,  7.11it/s]\n",
      "- Epoch: 12, Mode: validation, Loss: 0.00902, F-score : 0.232598: 100%|██████████| 445/445 [09:42<00:00,  1.31s/it]   \n",
      "- Epoch: 13, Mode: train, Loss: 0.009146 (0.006877),F-score : 0.236889 (0.245247): 100%|██████████| 4001/4001 [09:23<00:00,  7.09it/s]\n",
      "- Epoch: 13, Mode: validation, Loss: 0.009007, F-score : 0.233923: 100%|██████████| 445/445 [09:43<00:00,  1.31s/it]  \n",
      "- Epoch: 14, Mode: train, Loss: 0.00914 (0.006874),F-score : 0.237503 (0.254101): 100%|██████████| 4001/4001 [09:24<00:00,  7.09it/s] \n",
      "- Epoch: 14, Mode: validation, Loss: 0.009007, F-score : 0.232871: 100%|██████████| 445/445 [09:44<00:00,  1.31s/it]  \n",
      "- Epoch: 15, Mode: train, Loss: 0.009133 (0.00689),F-score : 0.237681 (0.244689): 100%|██████████| 4001/4001 [09:25<00:00,  7.08it/s] \n",
      "- Epoch: 15, Mode: validation, Loss: 0.009026, F-score : 0.242759: 100%|██████████| 445/445 [09:44<00:00,  1.31s/it]  \n"
     ]
    }
   ],
   "source": [
    "cafa_5_transformer = CAFA5Transformer(\n",
    "    amino_acids_vocab = cafa_5_train_data.seq_transform.vocab,\n",
    "    go_codes_vocab = cafa_5_train_data.go_codes_ids,\n",
    "    embedding_kwargs = {\n",
    "        \"embedding_dim\": 512,\n",
    "    },\n",
    "    transformer_kwargs = {\n",
    "        \"num_encoder_layers\": 2\n",
    "    },\n",
    "    feed_forward_kwargs = {\n",
    "        \"num_layers\": 0,\n",
    "        \"hidden_size\": 2048,\n",
    "        \"hidden_activation\": torch.nn.ReLU(),\n",
    "        \"dropout\": 0.1\n",
    "    }\n",
    ")\n",
    "cafa_5_transformer.to(\"cuda\")\n",
    "display(cafa_5_transformer)\n",
    "\n",
    "optimizer_kwargs = {\n",
    "    \"betas\": (0.9, 0.999),\n",
    "    \"eps\": 1e-08,\n",
    "    \"amsgrad\": True,\n",
    "}\n",
    "cafa_5_transformer.fit(\n",
    "    cafa_5_train_data,\n",
    "    epochs=16,\n",
    "    batch_size=32,\n",
    "    collate_fn = collate_tok_mult_out_batch,\n",
    "    loss_fn = ClassWeightedBCELoss(torch.tensor(cafa_5_train_data.go_codes_info_accr_weights, device=\"cuda\")),\n",
    "    optimizer_type = torch.optim.Adam,\n",
    "    optimizer_kwargs = optimizer_kwargs,\n",
    "    lr_scheduler_type = StepLRScheduler,\n",
    "    lr_scheduler_kwargs = {\n",
    "        \"d_model\": 512\n",
    "    },\n",
    "    verbose = True,\n",
    "    validation_size = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CAFA5Transformer(\n",
       "  (embedding): Embedding(28, 512)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (feed_forward): FCNN(\n",
       "    (hidden_activation): ReLU()\n",
       "    (output_activation): Sigmoid()\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=43248, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Epoch: 0, Mode: train, Loss: 0.011675 (0.01087),F-score : 0.238192 (0.142972): 100%|██████████| 4001/4001 [16:27<00:00,  4.05it/s] \n",
      "- Epoch: 0, Mode: validation, Loss: 0.009363, F-score : 0.216896: 100%|██████████| 445/445 [16:48<00:00,  2.27s/it]   \n",
      "- Epoch: 1, Mode: train, Loss: 0.009451 (0.010764),F-score : 0.236776 (0.143306): 100%|██████████| 4001/4001 [16:26<00:00,  4.06it/s]\n",
      "- Epoch: 1, Mode: validation, Loss: 0.009326, F-score : 0.216896: 100%|██████████| 445/445 [16:48<00:00,  2.27s/it]   \n",
      "- Epoch: 2, Mode: train, Loss: 0.009403 (0.010707),F-score : 0.23516 (0.143291): 100%|██████████| 4001/4001 [16:28<00:00,  4.05it/s] \n",
      "- Epoch: 2, Mode: validation, Loss: 0.009282, F-score : 0.216896: 100%|██████████| 445/445 [16:50<00:00,  2.27s/it]   \n",
      "- Epoch: 3, Mode: train, Loss: 0.009365 (0.010655),F-score : 0.234279 (0.192826): 100%|██████████| 4001/4001 [16:28<00:00,  4.05it/s]\n",
      "- Epoch: 3, Mode: validation, Loss: 0.009265, F-score : 0.255949: 100%|██████████| 445/445 [16:50<00:00,  2.27s/it]   \n",
      "- Epoch: 4, Mode: train, Loss: 0.009343 (0.010692),F-score : 0.234032 (0.184741): 100%|██████████| 4001/4001 [16:27<00:00,  4.05it/s]\n",
      "- Epoch: 4, Mode: validation, Loss: 0.009242, F-score : 0.255949: 100%|██████████| 445/445 [16:48<00:00,  2.27s/it]   \n",
      "- Epoch: 5, Mode: train, Loss: 0.009325 (0.010645),F-score : 0.233913 (0.196829): 100%|██████████| 4001/4001 [16:30<00:00,  4.04it/s]\n",
      "- Epoch: 5, Mode: validation, Loss: 0.009243, F-score : 0.205702: 100%|██████████| 445/445 [16:52<00:00,  2.28s/it]   \n",
      "- Epoch: 6, Mode: train, Loss: 0.009307 (0.010586),F-score : 0.233715 (0.168309): 100%|██████████| 4001/4001 [16:33<00:00,  4.03it/s]\n",
      "- Epoch: 6, Mode: validation, Loss: 0.010063, F-score : 0.205702: 100%|██████████| 445/445 [16:54<00:00,  2.28s/it]   \n",
      "- Epoch: 7, Mode: train, Loss: 0.00929 (0.010649),F-score : 0.233959 (0.157069): 100%|██████████| 4001/4001 [16:34<00:00,  4.02it/s] \n",
      "- Epoch: 7, Mode: validation, Loss: 0.010135, F-score : 0.205702: 100%|██████████| 445/445 [16:56<00:00,  2.28s/it]   \n",
      "- Epoch: 8, Mode: train, Loss: 0.00928 (0.010529),F-score : 0.234013 (0.185776): 100%|██████████| 4001/4001 [16:35<00:00,  4.02it/s] \n",
      "- Epoch: 8, Mode: validation, Loss: 0.010334, F-score : 0.205702: 100%|██████████| 445/445 [16:56<00:00,  2.29s/it]   \n",
      "- Epoch: 9, Mode: train, Loss: 0.009273 (0.010537),F-score : 0.23435 (0.173286): 100%|██████████| 4001/4001 [16:36<00:00,  4.02it/s] \n",
      "- Epoch: 9, Mode: validation, Loss: 0.010203, F-score : 0.205702: 100%|██████████| 445/445 [16:58<00:00,  2.29s/it]   \n",
      "- Epoch: 10, Mode: train, Loss: 0.009272 (0.01065),F-score : 0.23444 (0.146276): 100%|██████████| 4001/4001 [16:32<00:00,  4.03it/s]  \n",
      "- Epoch: 10, Mode: validation, Loss: 0.010466, F-score : 0.205466: 100%|██████████| 445/445 [16:54<00:00,  2.28s/it]   \n",
      "- Epoch: 11, Mode: train, Loss: 0.00926 (0.010483),F-score : 0.234193 (0.149789): 100%|██████████| 4001/4001 [16:34<00:00,  4.02it/s] \n",
      "- Epoch: 11, Mode: validation, Loss: 0.010293, F-score : 0.205466: 100%|██████████| 445/445 [16:56<00:00,  2.28s/it]   \n",
      "- Epoch: 12, Mode: train, Loss: 0.009257 (0.010521),F-score : 0.234608 (0.150079): 100%|██████████| 4001/4001 [16:34<00:00,  4.02it/s]\n",
      "- Epoch: 12, Mode: validation, Loss: 0.010482, F-score : 0.205466: 100%|██████████| 445/445 [16:56<00:00,  2.28s/it]   \n",
      "- Epoch: 13, Mode: train, Loss: 0.009257 (0.010482),F-score : 0.234242 (0.154694): 100%|██████████| 4001/4001 [16:37<00:00,  4.01it/s]\n",
      "- Epoch: 13, Mode: validation, Loss: 0.010732, F-score : 0.205466: 100%|██████████| 445/445 [16:58<00:00,  2.29s/it]   \n",
      "- Epoch: 14, Mode: train, Loss: 0.009253 (0.010413),F-score : 0.234458 (0.153254): 100%|██████████| 4001/4001 [16:37<00:00,  4.01it/s]\n",
      "- Epoch: 14, Mode: validation, Loss: 0.010832, F-score : 0.205466: 100%|██████████| 445/445 [16:59<00:00,  2.29s/it]   \n",
      "- Epoch: 15, Mode: train, Loss: 0.009254 (0.007213),F-score : 0.234371 (0.220854):  95%|█████████▍| 3792/4001 [15:48<00:52,  4.00it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 25\u001b[0m\n\u001b[1;32m     18\u001b[0m display(cafa_5_transformer)\n\u001b[1;32m     20\u001b[0m optimizer_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m     21\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m\"\u001b[39m: (\u001b[39m0.9\u001b[39m, \u001b[39m0.999\u001b[39m),\n\u001b[1;32m     22\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39meps\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1e-08\u001b[39m,\n\u001b[1;32m     23\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mamsgrad\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     24\u001b[0m }\n\u001b[0;32m---> 25\u001b[0m cafa_5_transformer\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     26\u001b[0m     cafa_5_train_data,\n\u001b[1;32m     27\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[1;32m     28\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[1;32m     29\u001b[0m     collate_fn \u001b[39m=\u001b[39;49m collate_tok_mult_out_batch,\n\u001b[1;32m     30\u001b[0m     loss_fn \u001b[39m=\u001b[39;49m ClassWeightedBCELoss(torch\u001b[39m.\u001b[39;49mtensor(cafa_5_train_data\u001b[39m.\u001b[39;49mgo_codes_info_accr_weights, device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m)),\n\u001b[1;32m     31\u001b[0m     optimizer_type \u001b[39m=\u001b[39;49m torch\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mAdam,\n\u001b[1;32m     32\u001b[0m     optimizer_kwargs \u001b[39m=\u001b[39;49m optimizer_kwargs,\n\u001b[1;32m     33\u001b[0m     lr_scheduler_type \u001b[39m=\u001b[39;49m StepLRScheduler,\n\u001b[1;32m     34\u001b[0m     lr_scheduler_kwargs \u001b[39m=\u001b[39;49m {\n\u001b[1;32m     35\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39md_model\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m512\u001b[39;49m\n\u001b[1;32m     36\u001b[0m     },\n\u001b[1;32m     37\u001b[0m     verbose \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     38\u001b[0m     validation_size \u001b[39m=\u001b[39;49m \u001b[39m0.1\u001b[39;49m\n\u001b[1;32m     39\u001b[0m )\n",
      "File \u001b[0;32m~/repositories/cafa_5/cafa_5/model.py:164\u001b[0m, in \u001b[0;36mTorchCAFA5Model.fit\u001b[0;34m(self, train_cafa_5_dataset, epochs, batch_size, collate_fn, loss_fn, loss_kwargs, optimizer_type, optimizer_kwargs, lr_scheduler_type, lr_scheduler_kwargs, validation_size, verbose)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39m# Compute loss and backpropagate loss gradient for each parameters\u001b[39;00m\n\u001b[1;32m    163\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(go_codes_probs_batch, data_batch[\u001b[39m\"\u001b[39m\u001b[39mgo_codes\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mfloat(), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mloss_kwargs_)\n\u001b[0;32m--> 164\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    166\u001b[0m \u001b[39m# Optimize the parameters according to the gradient and the optimizer\u001b[39;00m\n\u001b[1;32m    167\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/cafa_5/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/cafa_5/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cafa_5_transformer = CAFA5Transformer(\n",
    "    amino_acids_vocab = cafa_5_train_data.seq_transform.vocab,\n",
    "    go_codes_vocab = cafa_5_train_data.go_codes_ids,\n",
    "    embedding_kwargs = {\n",
    "        \"embedding_dim\": 512,\n",
    "    },\n",
    "    transformer_kwargs = {\n",
    "        \"num_encoder_layers\": 4\n",
    "    },\n",
    "    feed_forward_kwargs = {\n",
    "        \"num_layers\": 0,\n",
    "        \"hidden_size\": 2048,\n",
    "        \"hidden_activation\": torch.nn.ReLU(),\n",
    "        \"dropout\": 0.1\n",
    "    }\n",
    ")\n",
    "cafa_5_transformer.to(\"cuda\")\n",
    "display(cafa_5_transformer)\n",
    "\n",
    "optimizer_kwargs = {\n",
    "    \"betas\": (0.9, 0.999),\n",
    "    \"eps\": 1e-08,\n",
    "    \"amsgrad\": True,\n",
    "}\n",
    "cafa_5_transformer.fit(\n",
    "    cafa_5_train_data,\n",
    "    epochs=32,\n",
    "    batch_size=32,\n",
    "    collate_fn = collate_tok_mult_out_batch,\n",
    "    loss_fn = ClassWeightedBCELoss(torch.tensor(cafa_5_train_data.go_codes_info_accr_weights, device=\"cuda\")),\n",
    "    optimizer_type = torch.optim.Adam,\n",
    "    optimizer_kwargs = optimizer_kwargs,\n",
    "    lr_scheduler_type = StepLRScheduler,\n",
    "    lr_scheduler_kwargs = {\n",
    "        \"d_model\": 512\n",
    "    },\n",
    "    verbose = True,\n",
    "    validation_size = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cafa_5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
